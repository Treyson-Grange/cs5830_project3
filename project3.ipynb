{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3\n",
    "## Team members Arick Smith And Treyson Grange\n",
    "\n",
    "## Ideas\n",
    "\n",
    "- Scrape rolling stones for most mentioned artists\n",
    "- Scrape cooking recipes, find trends in ingredients and how expensive the recipes are\n",
    "- Working with the data creation from UCI mountain Bike world racing data set. (CHOOSEN)\n",
    "\n",
    "\n",
    "## TASKS\n",
    "- need to use the requests, Beautiful Soup, html5lib, and/or API libraries (e.g. Twitter, RottenTomatoes) to get web content.\n",
    "- Your analysis should include at least four techniques that we've discussed in class, and they should all contribute in a meaningful way to telling the story. \n",
    "- Make Project Report\n",
    "- Make Presentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports for the parsing of website data\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "\n",
    "#Common imports for use in the data visualization\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working on the parsing of website data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_web = \"https://ucimtbworldseries.com/teams\"\n",
    "\n",
    "site = requests.get(call_web)\n",
    "\n",
    "og_web_html = ''\n",
    "\n",
    "og_web_html = site.text\n",
    "print(og_web_html[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_matches = len(list(re.finditer('/teams/', str(og_web_html))))\n",
    "\n",
    "print(\"the total count of teams is likely half this number \", team_matches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(og_web_html)\n",
    "\n",
    "\n",
    "#Using the same code as the inclass example I found that the items classed as flex-shrink-0 would have our hyperlinks\n",
    "teams = [a['href'] for div in soup.find_all('div', {'class':'flex-shrink-0'}) for a in div.find_all('a')]\n",
    "# Removing the home dir\n",
    "teams.remove('/')\n",
    "print(teams)\n",
    "print(len(teams))\n",
    "\n",
    "#There are a total of 196 teams involved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Going to enter each team and make a compilition of the members that compose it (Getting each url to a team member)\n",
    "players_links = []\n",
    "\n",
    "for team in teams[:25]:\n",
    "    cur_team = requests.get(team)\n",
    "    if cur_team.status_code == 200:\n",
    "        sur_team_text = cur_team.text\n",
    "\n",
    "        team_soup = BeautifulSoup(sur_team_text)\n",
    "\n",
    "        current_links = [a['href'] for div in team_soup.find_all('div', {'id':'team-riders'}) for a in div.find_all('a')]\n",
    "\n",
    "        #Breaking apart links to have in one massive player link list\n",
    "        for i in current_links:\n",
    "            players_links.append(i)\n",
    "\n",
    "#All player links\n",
    "players_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getTopElements\n",
    "# input HTML Soup\n",
    "# Output: Team, Nationality, age, format, category\n",
    "def getTopElements(rider_soup):\n",
    "    divList = rider_soup.find('div', class_='flow-root bg-white')# this i gets the list at the top of the page\n",
    "    ul_element = divList.find('ul', class_='divide-y divide-gray-200 bg-white m-4')\n",
    "    output = []\n",
    "    output.append(rider_soup.find('p', class_='truncate text-xl font-headline text-gray-900 uppercase').text)\n",
    "    for li_element in ul_element.find_all('li'):\n",
    "        for div_element in li_element.find_all('div'):\n",
    "            if div_element.find('svg'):\n",
    "                continue\n",
    "            if not div_element.find('div'):\n",
    "                continue\n",
    "            \n",
    "            for p_element in div_element.find_all('p'):\n",
    "                if p_element.find('span'):\n",
    "                    continue\n",
    "                output.append(p_element.text)\n",
    "\n",
    "            \n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def getRiderResults(rider_soup):\n",
    "    allPlacements = []\n",
    "    table = rider_soup.find('table')\n",
    "    tbody = table.find('tbody')\n",
    "    for tr in tbody.find_all('tr'):\n",
    "        th_element = tr.find('td', class_='whitespace-nowrap py-4 pl-4 pr-3 text-center text-sm font-medium text-gray-900 sm:pl-0')\n",
    "        th_text = th_element.text.strip().rstrip('\\n')\n",
    "        allPlacements.append(th_text)\n",
    "    \n",
    "    return allPlacements\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following will create a rider entry to a csv file\n",
    "#This will contain a few of the following peices of data: NAME,AGE,NATIONALITY,CATEGORY, etc...\n",
    "with open('riders.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    filewriter = csv.writer(csvfile, delimiter=',')\n",
    "    filewriter.writerow(['Name', 'Age', 'Nationality','Team', 'Category', 'Formats', 'AVG_Place', 'TTL_First', 'TTL_Top_Ten', 'TTL_Races_2023', 'AVG_T_Behind_First', 'XCOWCP', 'XCCWCP'])\n",
    "    \n",
    "    for rider in players_links:\n",
    "        # print(rider)\n",
    "        cur_rider = requests.get(rider)\n",
    "        if cur_rider.status_code == 200:\n",
    "            cur_rider_html = cur_rider.text\n",
    "            rider_soup = BeautifulSoup(cur_rider_html)\n",
    "\n",
    "            # For name, age, nationality, category, and formats\n",
    "            output = getTopElements(rider_soup)\n",
    "            print(output)\n",
    "            # For placements of every race\n",
    "            allPlacements = getRiderResults(rider_soup)\n",
    "            \n",
    "            # For AVG_Place\n",
    "            total = 0\n",
    "            while \"DNF\" in allPlacements:\n",
    "                allPlacements.remove(\"DNF\")  # Remove \"DNF\" from the list\n",
    "            while \"DSQ\" in allPlacements:\n",
    "                allPlacements.remove(\"DSQ\")  # Remove \"DNF\" from the list\n",
    "            while \"DNS\" in allPlacements:\n",
    "                allPlacements.remove(\"DNS\")  # Remove \"DNS\" from the list\n",
    "            for num in allPlacements:\n",
    "                try:\n",
    "                    total += float(num)\n",
    "                except ValueError:\n",
    "                    pass  # Ignore non-numeric values like \"DNS\"\n",
    "            \n",
    "            count = len(allPlacements)\n",
    "            #For TTL_First and TTL_Top_Ten\n",
    "            TTLFirstCounter = 0\n",
    "            TTLTop10Counter = 0\n",
    "            for item in allPlacements:\n",
    "                if int(item)<= 1:\n",
    "                    TTLFirstCounter += 1\n",
    "                if int(item)<= 10:\n",
    "                    TTLTop10Counter += 1\n",
    "\n",
    "\n",
    "            formatCleaned = output[4].split(',')\n",
    "            categoryCleaned = output[5].split(',')\n",
    "            if count == 0:\n",
    "                average = 0\n",
    "            else:    \n",
    "                average = round(total / count)\n",
    "\n",
    "\n",
    "            try:\n",
    "                filewriter.writerow([output[0].rstrip('\\n').strip(), output[5].rstrip('\\n').strip(), output[3].rstrip('\\n').strip(), output[1].rstrip(\"\\n\").strip(), output[9].rstrip('\\n').strip(), output[7].strip().rstrip('\\n'), average, TTLFirstCounter, TTLTop10Counter, len(allPlacements), 0, 0, 0])\n",
    "            except IndexError:\n",
    "                pass\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transition from gathering Data to Interpreting the data\n",
    "## Questions we seek to answer\n",
    "\n",
    "- Whos the nation to beat in world series Mountain Biking?\n",
    "- Does America Beat out the British?\n",
    "- What contributes to riders getting First Place?\n",
    "- Does a DNF mean they are an often poor racer? (Bottom half of leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in the data from our created data set. This ensures it works with the code without the need to make the CSV file every time\n",
    "\n",
    "rider_df = pd.read_csv('riders.csv', encoding='utf-8')\n",
    "rider_df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab all of the rider info for the brits for later use\n",
    "\n",
    "rider_UK = rider_df[rider_df['Nationality'] == \"United Kingdom\"]\n",
    "rider_UK.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab all of the rider info for the yanks for later use\n",
    "\n",
    "rider_USA = rider_df[rider_df['Nationality'] == \"United States\"]\n",
    "rider_USA.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before only comparing the US and UK Lets find out which nation is running the best track record \n",
    "\n",
    "nationalities = rider_df['Nationality'].unique()\n",
    "nationalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_place_count = {}\n",
    "\n",
    "for nation in nationalities:\n",
    "    nation_df = rider_df[rider_df['Nationality'] == nation]\n",
    "    nation_first_place = nation_df['TTL_First'].sum()\n",
    "    nation_TTL_races = nation_df['TTL_Races_2023'].sum()\n",
    "\n",
    "    percent_of_races_first = nation_first_place/nation_TTL_races * 100\n",
    "\n",
    "    print(f'Total First places is: {nation_first_place}|| In total {nation} racers competed in {nation_TTL_races} in 2023. As a nation they get first {percent_of_races_first}% of the time')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
